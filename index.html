
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 22px;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 32px;
}

.one {
width: 160px;
height: 160px;
position: relative;
}

.two {
width: 160px;
height: 180px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.img {
    /* max-width: 100%; */
    /* max-height: 90%; */
    width: 200px;
}

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
  <link rel="icon" type="image/png" href="imgs/icon.png">
  <title>Zhenzhen (Jen) Weng</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  </head>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8F02WVX4VF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'G-8F02WVX4VF');
    </script>

    <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=QW0v2Saws-oYCnI5w1P_wpyTQno_xA66F2dpqjo2aV4&cl=ffffff&w=a"></script> -->

  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="90%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="90%" valign="middle">
        <p align="center">
          <name>Zhenzhen Weng </name>
        </p>
        <p>I am a ML engineer at Waymo Perception working on multi-modal (image, video, text) foundation models for self-driving.</p>
        <p>I received my Ph.D. in Computational and Mathematical Engineering (<a href="https://icme.stanford.edu/">ICME</a>) from Stanford University where I was advised by <a href="https://ai.stanford.edu/~syyeung/">Prof. Serena Yeung</a>. </p>

        <p>I am broadly interested in computer vision and machine learning. My PhD research was focused on human-centric 3D perception and generative models. I interned at Waymo Research where I worked on human-centric representation learning from LiDAR data, as well as Adobe Research where I worked on generalizable single-view human NeRF prediction. </p>

        <p>Prior to my Ph.D, I received B.S. in Computer Science and B.S. in Mathematics from <a href="https://www.cmu.edu/">Carnegie Mellon University</a>. I also previously worked as a Research Engineer for a fund manager on the East Coast, on large-scale backtesting and portfolio optimization services. </p>
	
        <p align=center>
          <a href="mailto:zzweng@stanford.edu">Email</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/jen-weng-13616154/">Linkedin</a> &nbsp/&nbsp
          <a href="https://twitter.com/JenWeng4">Twitter</a>  &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=diDrNrgAAAAJ&hl=en">Google Scholar</a>
        </p>
        </td>
      </tr>
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>News</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr onmouseout="box_stop()" onmouseover="box_start()">
    
        <td valign="center" width="75%">
          <p>
      <b>Jan, 2025: </b> 1 research article "Artificial Intelligence-Powered 3D Analysis of Video-Based Caregiver-Child Interactions" accepted to <a href="https://www.science.org/journal/sciadv">Science Advances</a>.
            <br> <br>
	    <b>June, 2024: </b> Joined Waymo Perception to work on large multi-modal foundation models for self-driving.
            <br> <br>
	    <b>May, 2024: </b> Defended my dissertation: <a href="https://searchworks.stanford.edu/view/in00000106991">Human-Centric Perception with Limited Supervision: Improving Generalizability In the Wild</a>.
            <br> <br>
	    <!-- <b>Mar, 2024: </b> Attended 3DV 24 in Davos, Switzerland. -->
	    <!-- <br> <br> -->
	    <b>Oct, 2023: </b> 
            1 paper accepted to 3DV 2024.
            <br> <br>
            <b>June, 2023: </b> 
            Attended CVPR 2023 in Vancouver, Canada.
            <br> <br>
            <b>Feb, 2023: </b> 
            <!-- <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">  -->
            2 papers accepted to CVPR 2023. 
            <!-- <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> -->
            <br> <br>
          </p>
        </td>
      </tr>

      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research and Publications</heading>
          <!-- <p>
            * denotes equal contribution co-authorship
          </p> -->
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

            <tr>
              <td width="35%">
                <div class="one">
                  <div class="two"><img src='imgs/teasers/HumanLRM_v2.png' class="img"></div>
                  <!-- <img src='imgs/diffusion_hpc.png' width="160"> -->
                </div>
              </td>
              <td valign="top" width="75%">
                <a href="https://zzweng.github.io/humanlrm/">
                  <papertitle>Template-Free Single-View 3D Human Digitalization with Diffusion-Guided LRM</papertitle>
                </a>
                <br>
                <strong>Zhenzhen Weng</strong>, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, Jimei Yang
                <br>
                <a href="https://arxiv.org/abs/2401.12175">Preprint</a> | 
                  <a href="https://zzweng.github.io/humanlrm/">Website</a>
                <br>
                <!-- <a href="https://arxiv.org/pdf/2212.13660.pdf">Paper</a> -->
                <p></p>
                <p>Reconstructing 3D humans from a single image has been extensively investigated. However, existing approaches often fall short on capturing fine geometry and appearance details, hallucinating occluded parts with plausible details, and achieving generalization across unseen and in-the-wild datasets.
                  We present Human-LRM, a diffusion-guided feed-forward model that predicts the implicit field of a human from a single image. Leveraging the power of the state-of-the-art reconstruction model (i.e., LRM) and generative model (i.e Stable Diffusion), our method is able to capture human without any template prior, e.g., SMPL, and effectively enhance occluded parts with rich and realistic details.</p>
              </td>
            </tr>

            <tr>
              <td width="35%">
                <div class="one">
                  <div class="two"><img src='imgs/teasers/Slide8.png' class="img"></div>
                  <!-- <img src='imgs/diffusion_hpc.png' width="160"> -->
                </div>
              </td>
              <td valign="top" width="75%">
                <a href="">
                  <papertitle>Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains </papertitle>
                </a>
                <br>
                <strong>Zhenzhen Weng</strong>, Laura Bravo, Serena Yeung
                <br>
                <a href="http://arxiv.org/abs/2303.09541">3DV 2024 (Spotlight)</a> | 
                  <a href="projects/diffusion_hpc.html">Website</a> | 
                  <a href="https://github.com/ZZWENG/Diffusion_HPC">Code</a>
                <br>
                <!-- <a href="https://arxiv.org/pdf/2212.13660.pdf">Paper</a> -->
                <p></p>
                <p>Recent text-to-image generative models such as Stable Diffusion often struggle to preserve plausible human structure in the generations. We propose Diffusion model with Human Pose Correction (Diffusion-HPC),
                a method that generates photo-realistic images with plausible posed humans by injecting prior knowledge about human body structure. The generated image-mesh pairs are well-suited for downstream human mesh recovery task.</p>
              </td>
            </tr>

            <tr>
              <td width="35%">
                <div class="one">
                  <div class="two"><img src='imgs/teasers/slide9.png' class="img"></div>
                  <!-- <img src='imgs/diffusion_hpc.png' width="160"> -->
                </div>
              </td>
              <td valign="top" width="75%">
                <a href="">
                  <papertitle>ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image</papertitle>
                </a>
                <br>
                <strong>Zhenzhen Weng</strong>, Zeyu Wang, Serena Yeung
                <br>
                <a href="https://arxiv.org/pdf/2305.16411">Preprint</a> | 
                  <a href="projects/zeroavatar.html">Website</a>
                <br>
                <p></p>
                <p>We present ZeroAvatar, a method that introduces the explicit 3D human body prior to the optimization process. We show that ZeroAvatar significantly enhances the robustness and 3D consistency of optimization-based image-to-3D avatar generation, outperforming existing zero-shot image-to-3D methods.</p>
              </td>
            </tr>

            <tr>
              <td width="35%">
                <div class="one">
                  <div class="two"><img src='imgs/teasers/Slide7.png' width="200"></div>
                  <!-- <img src='imgs/uhkp.png' width="120"> -->
                </div>
              </td>
              <td valign="top" width="75%">
                <a href="">
                  <papertitle>3D Human Keypoints Estimation from Point Clouds in the Wild without Human Labels</papertitle>
                </a>
                <br>
                <strong>Zhenzhen Weng</strong>, Alexander S. Gorban, Jingwei Ji, Mahyar Najibi, Yin Zhou, Dragomir Anguelov
                <br>
                <i>Conference on Computer Vision and Pattern Recognition (CVPR), 2023</i>
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_3D_Human_Keypoints_Estimation_From_Point_Clouds_in_the_Wild_CVPR_2023_paper.pdf">Paper</a>
                | <a href="https://cvpr2023.thecvf.com/virtual/2023/poster/21046">Project</a>
                <p></p>
                <p>We propose GC-KPL - Geometry Consistency inspired Key Point Leaning. By training on the large WOD training set without any annotated keypoints, we attain reasonable performance as compared to the fully supervised approach. Further, the backbone benefits from the unsupervised training and is useful in downstream fewshot learning of keypoints, where fine-tuning on only 10 percent of the labeled training data gives comparable performance to fine-tuning on the entire set.</p>
              </td>
            </tr>

            <tr>
            <td width="35%">
              <div class="one">
                <div class="two"><img src='imgs/teasers/Slide6.png' class="img" ></div>
                <!-- <img src='imgs/nemo.png' width="160"> -->
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2212.13660.pdf">
                <papertitle>NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action</papertitle>
              </a>
              <br>
              Kuan-Chieh Wang, <strong>Zhenzhen Weng</strong>, Maria Xenochristou, Joao Pedro Araujo, Jeffrey Gu, C. Karen Liu, Serena Yeung
              <br>
              <i>Conference on Computer Vision and Pattern Recognition (CVPR) (Highlight), 2023</i>
              <br>
              <a href="https://arxiv.org/pdf/2212.13660.pdf">Paper</a> | <a href="https://sites.google.com/view/nemo-neural-motion-field">Website</a>
              <p></p>
              <p>We aim to bridge the gap between monocular HMR and multi-view MoCap systems by leveraging information shared across multiple video instances of the same action. We introduce the Neural Motion (NeMo) field. It is optimized to represent the underlying 3D motions across a set of videos of the same action.</p>
            </td>
          </tr>
          <!-- <tr onmouseout="lfd_stop()" onmouseover="lfd_start()"> -->
            <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='imgs/teasers/Slide5.png' class="img" ></div>
                <!-- <img src='imgs/dapa.png' width="160"> -->
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2206.10457">
                <papertitle>Domain Adaptive 3D Pose Augmentation for In-the-wild Human Mesh Recovery</papertitle>
              </a>
              <br>
              <strong>Zhenzhen Weng</strong>,
              <a href="https://wangkua1.github.io/">Kuan-Chieh (Jackson) Wang</a>,
              <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
              <a href="https://scholar.google.com/citations?user=Tw2m5kUAAAAJ&hl=en&oi=ao">Serena Yeung</a>
              <br>
              <em>International Conference on 3D Vision <a href="https://3dvconf.github.io/2022/">(3DV)</a></em>, 2022
              <!-- <em>CVPR Workshop on Computer Vision for Augmented and Virtual Reality <a href="https://xr.cornell.edu/workshop/2022/papers">(CV4ARVR)</a></em>, 2022 -->
	            <br>
              <a href="https://arxiv.org/abs/2206.10457">Paper</a> | <a href="projects/dapa.html"> Project Page</a> | <a href="https://github.com/ZZWENG/DAPA_release">Code</a>
              <p></p>
              <p>We propose Domain Adaptive 3D Pose Augmentation (DAPA), a data augmentation method that combines the strength of methods based on synthetic datasets by getting direct supervision from the synthesized meshes.</p>
            </td>
          </tr>
          
          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='holistic_image'><img src='imgs/teasers/Slide4.png' class="img" ></div>
                <!-- <img src='imgs/holistic.png' width="160"> -->
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2012.01591.pdf">
                <papertitle>Holistic 3D Human and Scene Mesh Estimation from Single View Images</papertitle>
              </a>
              <br>
              <strong>Zhenzhen Weng</strong>,
              <a href="https://scholar.google.com/citations?user=Tw2m5kUAAAAJ&hl=en&oi=ao">Serena Yeung</a>
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2012.01591.pdf">Paper</a>
              <p></p>
              <p>We propose a holistically trainable model that perceives the 3D
                scene from a single RGB image, estimates the camera pose
                and the room layout, and reconstructs both human body
                and object meshes. </p>
            </td>
          </tr>
          
          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='longtail_image'><img src='imgs/teasers/Slide3.png' class="img" ></div>
                <!-- <img src='imgs/longtail.png' width="160"> -->
              </div>
              <!-- <script type="text/javascript">
                function lfd_start() {
                  document.getElementById('lfd_image').style.opacity = "1";
                }

                function lfd_stop() {
                  document.getElementById('lfd_image').style.opacity = "0";
                }
                lfd_stop()
              </script> -->
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2104.01257">
                <papertitle>Unsupervised Discovery of the Long-Tail in Instance Segmentation Using Hierarchical Self-Supervision</papertitle>
              </a>
              <br>
              <strong>Zhenzhen Weng</strong>,
              Mehmet Giray Ogut,
              Shai Limonchik,
              <a href="https://scholar.google.com/citations?user=Tw2m5kUAAAAJ&hl=en&oi=ao">Serena Yeung</a>
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2104.01257">Paper</a>
              <p></p>
              <p>We propose a method that can perform unsupervised discovery of long-tail categories in instance segmentation, through learning instance embeddings of masked regions. </p>
            </td>
          </tr>

          <tr onmouseout="dd_stop()" onmouseover="dd_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='slice_image'><img src='imgs/teasers/Slide2.png' class="img"></div>
                <!-- <img src='imgs/slice_based_learning.png' width="160"> -->
              </div>
             
            </td>
            <td valign="top" width="75%">
              <a href="https://papers.nips.cc/paper/2019/file/351869bde8b9d6ad1e3090bd173f600d-Paper.pdf">
                <papertitle>Slice-based learning: A programming model for residual learning in critical data slices</papertitle>
              </a>
              <br>
              <a href="https://vincentsc.com/">Vincent S Chen</a>, 
              <a href="https://scholar.google.com/citations?user=PzoN2hgAAAAJ&hl=en">Sen Wu</a>,
              <strong>Zhenzhen Weng</strong>,
              <a href="https://ajratner.github.io/">Alexander Ratner</a>,
              <a href="https://cs.stanford.edu/~chrismre/">Christopher R&eacute;</a>
              <br>
              <em>The Conference and Workshop on Neural Information Processing Systems (NeurIPS)</em>, 2019
              <br>
              <a href="https://papers.nips.cc/paper/2019/file/351869bde8b9d6ad1e3090bd173f600d-Paper.pdf">Paper</a>
              <p></p>
              <p>We introduce the challenge of improving slice-specific performance without damaging the overall model quality, and proposed the first programming abstraction and machine learning model to support
                these actions.</p>
            </td>
          </tr>

          <tr onmouseout="seg_stop()" onmouseover="seg_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='cyclist_image'><img src='imgs/teasers/Slide1.png' width="200"></div>
                <!-- <img src='imgs/cyclist.png' width="160"> -->
              </div>
            </td>
            
            <td valign="top" width="75%">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814147&tag=1">
                <papertitle>Utilizing Weak Supervision to Infer Complex Objects and Situations in Autonomous Driving Data </papertitle>
              </a>
              <br>
              <strong>Zhenzhen Weng</strong>,
              <a href="http://paroma.github.io/">Paroma Varma</a>,
              <a href="https://www.linkedin.com/in/masalov/">Alexander Masalov</a>,
              <a href="https://www.linkedin.com/in/jeffota/">Jeffrey Ota</a>,
              <a href="https://cs.stanford.edu/~chrismre/">Christopher R&eacute;</a>
              <br>
              <em>IEEE Intelligent Vehicles Symposium<a href="https://iv2019.org/"> (IEEE IV)</a></em>, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814147&tag=1">Paper</a>
              <p></p>
              <p>We introduced weak supervision heuristics as a methodology to infer complex objects and situations by combining simpler outputs from current, state-of-the art object detectors.</p>
            </td>
          </tr>

      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Work Experience</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr onmouseout="box_stop()" onmouseover="box_start()">
    
        <td valign="center" width="75%">
          <p>
            Research Scientist Intern @ <a href="https://research.adobe.com/">Adobe Research</a>, San Jose, CA, Jun - Sept, 2023
            <br><br>

            Research Intern (Perception) @ <a href="https://waymo.com/?ncr">Waymo</a>, Mountain View, CA, Jun - Nov, 2022
            <br><br>

            Machine Learning Engineer @ <a href="https://www.vmware.com/">VMware</a>, Palo Alto, CA, Jun - Sept, 2019
            <br><br>
            
            Research Engineer  @ <a href="https://www.aqr.com/">AQR</a>, Greenwich, CT, 2016 - 2018
            <br><br>

            </a>
            <br>
          </p>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellpadding="20">
        <tr>
          <a href='https://clustrmaps.com/site/1byfu'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=QW0v2Saws-oYCnI5w1P_wpyTQno_xA66F2dpqjo2aV4&co=90c9f2&ct=ffffff'/></a>
        </tr>
        
      </table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          Webpage template and source code from <a href="https://jonbarron.info">Jon Barron</a>. 
	    </font>
        </p>
        </td>
      </tr>
      </table>

    </td>
    </tr>
  </table>
  </body>
</html>
