
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 22px;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 32px;
}

.one {
width: 160px;
height: 160px;
position: relative;
}

.two {
width: 160px;
height: 180px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.img {
    /* max-width: 100%; */
    /* max-height: 90%; */
    width: 200px;
}

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
  <link rel="icon" type="image/png" href="imgs/icon.png">
  <title>Zhenzhen (Jen) Weng</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  </head>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8F02WVX4VF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
  
      gtag('config', 'G-8F02WVX4VF');
    </script>

  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="90%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="90%" valign="middle">
        <p align="center">
          <name>Zhenzhen (Jen) Weng</name>
        </p>
       
        <p>I am a final year Ph.D student in Computational and Mathematical Engineering (<a href="https://icme.stanford.edu/">ICME</a>) at Stanford University where I am advised by <a href="https://ai.stanford.edu/~syyeung/">Prof. Serena Yeung</a>. </p>

        <p>I am broadly interested in 3D computer vision. Specifically, I am interested in human-centric 3D perception and generative models. I have interned at Waymo Research in 2022 where I worked on efficient human 3D pose estimation from LiDAR data. In 2023, I interned at Adobe Research and worked on generalizable single-view human NeRF prediction. </p>

        <p>Prior to my Ph.D, I received B.S. in Computer Science and B.S. in Mathematics from <a href="https://www.cmu.edu/">Carnegie Mellon University</a>. I also previously worked for a fund manager on the East Coast.</p>

        <p style="color:red;">I will be graduating in June 2024 and I am looking for research scientist roles in 3D computer vision.</p>
        
        <p align=center>
          <a href="mailto:zzweng@stanford.edu">Email</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/jen-weng-13616154/">Linkedin</a> &nbsp/&nbsp
          <a href="https://twitter.com/JenWeng4">Twitter</a>  &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=diDrNrgAAAAJ&hl=en">Google Scholar</a>
        </p>
        </td>
      </tr>
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>News</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr onmouseout="box_stop()" onmouseover="box_start()">
    
        <td valign="center" width="75%">
          <p>
            <b>Oct, 2023: </b> 
            1 paper accepted to 3DV 2024. See you in Davos, Switzerland!
            <br> <br>
            <b>June, 2023: </b> 
            Attended CVPR 2023 in Vancouver, Canada.
            <br> <br>
            <b>Feb, 2023: </b> 
            <!-- <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">  -->
            2 papers accepted to CVPR 2023. 
            <!-- <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> -->
            <br> <br>
            <b>Jan, 2023: </b> 
            <!-- <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> -->
            Joining Adobe Research as a Research Scientist Intern this summer.  
            <!-- <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> -->
            <br> <br>
           
          </p>
        </td>
      </tr>

      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research and Publications</heading>
          <!-- <p>
            * denotes equal contribution co-authorship
          </p> -->
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

            <tr>
              <td width="35%">
                <div class="one">
                  <div class="two"><img src='imgs/teasers/HumanLRM.png' class="img"></div>
                  <!-- <img src='imgs/diffusion_hpc.png' width="160"> -->
                </div>
              </td>
              <td valign="top" width="75%">
                <a href="https://zzweng.github.io/humanlrm/">
                  <papertitle>Single-View 3D Human Digitalization with Large Reconstruction Models</papertitle>
                </a>
                <br>
                <strong>Zhenzhen Weng</strong>, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, Jimei Yang
                <br>
                <a href="">Preprint</a> | 
                  <a href="https://zzweng.github.io/humanlrm/">Website</a>
                <br>
                <!-- <a href="https://arxiv.org/pdf/2212.13660.pdf">Paper</a> -->
                <p></p>
                <p>We introduce Human-LRM, a single-stage feed-forward Large Reconstruction Model designed to predict human Neural Radiance Fields (NeRF) from a single image. Our approach demonstrates remarkable adaptability in training using extensive datasets containing 3D scans and multi-view capture. Furthermore, to enhance the model's applicability for in-the-wild scenarios especially with occlusions, we propose a novel strategy that distills multi-view reconstruction into single-view via a conditional triplane diffusion model. </p>
              </td>
            </tr>

            <tr>
              <td width="35%">
                <div class="one">
                  <div class="two"><img src='imgs/teasers/Slide8.png' class="img"></div>
                  <!-- <img src='imgs/diffusion_hpc.png' width="160"> -->
                </div>
              </td>
              <td valign="top" width="75%">
                <a href="">
                  <papertitle>Diffusion-HPC: Synthetic Data Generation for Human Mesh Recovery in Challenging Domains </papertitle>
                </a>
                <br>
                <strong>Zhenzhen Weng</strong>, Laura Bravo, Serena Yeung
                <br>
                <a href="http://arxiv.org/abs/2303.09541">3DV 2024 (Spotlight)</a> | 
                  <a href="projects/diffusion_hpc.html">Website</a> | 
                  <a href="https://github.com/ZZWENG/Diffusion_HPC">Code</a>
                <br>
                <!-- <a href="https://arxiv.org/pdf/2212.13660.pdf">Paper</a> -->
                <p></p>
                <p>Recent text-to-image generative models such as Stable Diffusion often struggle to preserve plausible human structure in the generations. We propose Diffusion model with Human Pose Correction (Diffusion-HPC),
                a method that generates photo-realistic images with plausible posed humans by injecting prior knowledge about human body structure. The generated image-mesh pairs are well-suited for downstream human mesh recovery task.</p>
              </td>
            </tr>

            <tr>
              <td width="35%">
                <div class="one">
                  <div class="two"><img src='imgs/teasers/slide9.png' class="img"></div>
                  <!-- <img src='imgs/diffusion_hpc.png' width="160"> -->
                </div>
              </td>
              <td valign="top" width="75%">
                <a href="">
                  <papertitle>ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image</papertitle>
                </a>
                <br>
                <strong>Zhenzhen Weng</strong>, Zeyu Wang, Serena Yeung
                <br>
                <a href="https://arxiv.org/pdf/2305.16411">Preprint</a> | 
                  <a href="projects/zeroavatar.html">Website</a>
                <br>
                <p></p>
                <p>We present ZeroAvatar, a method that introduces the explicit 3D human body prior to the optimization process. We show that ZeroAvatar significantly enhances the robustness and 3D consistency of optimization-based image-to-3D avatar generation, outperforming existing zero-shot image-to-3D methods.</p>
              </td>
            </tr>

            <tr>
              <td width="35%">
                <div class="one">
                  <div class="two"><img src='imgs/teasers/Slide7.png' width="200"></div>
                  <!-- <img src='imgs/uhkp.png' width="120"> -->
                </div>
              </td>
              <td valign="top" width="75%">
                <a href="">
                  <papertitle>3D Human Keypoints Estimation from Point Clouds in the Wild without Human Labels</papertitle>
                </a>
                <br>
                <strong>Zhenzhen Weng</strong>, Alexander S. Gorban, Jingwei Ji, Mahyar Najibi, Yin Zhou, Dragomir Anguelov
                <br>
                <i>Conference on Computer Vision and Pattern Recognition (CVPR), 2023</i>
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_3D_Human_Keypoints_Estimation_From_Point_Clouds_in_the_Wild_CVPR_2023_paper.pdf">Paper</a>
                | <a href="https://cvpr2023.thecvf.com/virtual/2023/poster/21046">Project</a>
                <p></p>
                <p>We propose GC-KPL - Geometry Consistency inspired Key Point Leaning. By training on the large WOD training set without any annotated keypoints, we attain reasonable performance as compared to the fully supervised approach. Further, the backbone benefits from the unsupervised training and is useful in downstream fewshot learning of keypoints, where fine-tuning on only 10 percent of the labeled training data gives comparable performance to fine-tuning on the entire set.</p>
              </td>
            </tr>

            <tr>
            <td width="35%">
              <div class="one">
                <div class="two"><img src='imgs/teasers/Slide6.png' class="img" ></div>
                <!-- <img src='imgs/nemo.png' width="160"> -->
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2212.13660.pdf">
                <papertitle>NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action</papertitle>
              </a>
              <br>
              Kuan-Chieh Wang, <strong>Zhenzhen Weng</strong>, Maria Xenochristou, Joao Pedro Araujo, Jeffrey Gu, C. Karen Liu, Serena Yeung
              <br>
              <i>Conference on Computer Vision and Pattern Recognition (CVPR) (Highlight), 2023</i>
              <br>
              <a href="https://arxiv.org/pdf/2212.13660.pdf">Paper</a> | <a href="https://sites.google.com/view/nemo-neural-motion-field">Website</a>
              <p></p>
              <p>We aim to bridge the gap between monocular HMR and multi-view MoCap systems by leveraging information shared across multiple video instances of the same action. We introduce the Neural Motion (NeMo) field. It is optimized to represent the underlying 3D motions across a set of videos of the same action.</p>
            </td>
          </tr>
          <!-- <tr onmouseout="lfd_stop()" onmouseover="lfd_start()"> -->
            <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='imgs/teasers/Slide5.png' class="img" ></div>
                <!-- <img src='imgs/dapa.png' width="160"> -->
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2206.10457">
                <papertitle>Domain Adaptive 3D Pose Augmentation for In-the-wild Human Mesh Recovery</papertitle>
              </a>
              <br>
              <strong>Zhenzhen Weng</strong>,
              <a href="https://wangkua1.github.io/">Kuan-Chieh (Jackson) Wang</a>,
              <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
              <a href="https://scholar.google.com/citations?user=Tw2m5kUAAAAJ&hl=en&oi=ao">Serena Yeung</a>
              <br>
              <em>International Conference on 3D Vision <a href="https://3dvconf.github.io/2022/">(3DV)</a></em>, 2022
              <!-- <em>CVPR Workshop on Computer Vision for Augmented and Virtual Reality <a href="https://xr.cornell.edu/workshop/2022/papers">(CV4ARVR)</a></em>, 2022 -->
	            <br>
              <a href="https://arxiv.org/abs/2206.10457">Paper</a> | <a href="projects/dapa.html"> Project Page</a> | <a href="https://github.com/ZZWENG/DAPA_release">Code</a>
              <p></p>
              <p>We propose Domain Adaptive 3D Pose Augmentation (DAPA), a data augmentation method that combines the strength of methods based on synthetic datasets by getting direct supervision from the synthesized meshes.</p>
            </td>
          </tr>
          
          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='holistic_image'><img src='imgs/teasers/Slide4.png' class="img" ></div>
                <!-- <img src='imgs/holistic.png' width="160"> -->
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2012.01591.pdf">
                <papertitle>Holistic 3D Human and Scene Mesh Estimation from Single View Images</papertitle>
              </a>
              <br>
              <strong>Zhenzhen Weng</strong>,
              <a href="https://scholar.google.com/citations?user=Tw2m5kUAAAAJ&hl=en&oi=ao">Serena Yeung</a>
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2012.01591.pdf">Paper</a>
              <p></p>
              <p>We propose a holistically trainable model that perceives the 3D
                scene from a single RGB image, estimates the camera pose
                and the room layout, and reconstructs both human body
                and object meshes. </p>
            </td>
          </tr>
          
          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='longtail_image'><img src='imgs/teasers/Slide3.png' class="img" ></div>
                <!-- <img src='imgs/longtail.png' width="160"> -->
              </div>
              <!-- <script type="text/javascript">
                function lfd_start() {
                  document.getElementById('lfd_image').style.opacity = "1";
                }

                function lfd_stop() {
                  document.getElementById('lfd_image').style.opacity = "0";
                }
                lfd_stop()
              </script> -->
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/abs/2104.01257">
                <papertitle>Unsupervised Discovery of the Long-Tail in Instance Segmentation Using Hierarchical Self-Supervision</papertitle>
              </a>
              <br>
              <strong>Zhenzhen Weng</strong>,
              Mehmet Giray Ogut,
              Shai Limonchik,
              <a href="https://scholar.google.com/citations?user=Tw2m5kUAAAAJ&hl=en&oi=ao">Serena Yeung</a>
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2104.01257">Paper</a>
              <p></p>
              <p>We propose a method that can perform unsupervised discovery of long-tail categories in instance segmentation, through learning instance embeddings of masked regions. </p>
            </td>
          </tr>

          <tr onmouseout="dd_stop()" onmouseover="dd_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='slice_image'><img src='imgs/teasers/Slide2.png' class="img"></div>
                <!-- <img src='imgs/slice_based_learning.png' width="160"> -->
              </div>
             
            </td>
            <td valign="top" width="75%">
              <a href="https://papers.nips.cc/paper/2019/file/351869bde8b9d6ad1e3090bd173f600d-Paper.pdf">
                <papertitle>Slice-based learning: A programming model for residual learning in critical data slices</papertitle>
              </a>
              <br>
              <a href="https://vincentsc.com/">Vincent S Chen</a>, 
              <a href="https://scholar.google.com/citations?user=PzoN2hgAAAAJ&hl=en">Sen Wu</a>,
              <strong>Zhenzhen Weng</strong>,
              <a href="https://ajratner.github.io/">Alexander Ratner</a>,
              <a href="https://cs.stanford.edu/~chrismre/">Christopher R&eacute;</a>
              <br>
              <em>The Conference and Workshop on Neural Information Processing Systems (NeurIPS)</em>, 2019
              <br>
              <a href="https://papers.nips.cc/paper/2019/file/351869bde8b9d6ad1e3090bd173f600d-Paper.pdf">Paper</a>
              <p></p>
              <p>We introduce the challenge of improving slice-specific performance without damaging the overall model quality, and proposed the first programming abstraction and machine learning model to support
                these actions.</p>
            </td>
          </tr>

          <tr onmouseout="seg_stop()" onmouseover="seg_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='cyclist_image'><img src='imgs/teasers/Slide1.png' width="200"></div>
                <!-- <img src='imgs/cyclist.png' width="160"> -->
              </div>
            </td>
            
            <td valign="top" width="75%">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814147&tag=1">
                <papertitle>Utilizing Weak Supervision to Infer Complex Objects and Situations in Autonomous Driving Data </papertitle>
              </a>
              <br>
              <strong>Zhenzhen Weng</strong>,
              <a href="http://paroma.github.io/">Paroma Varma</a>,
              <a href="https://www.linkedin.com/in/masalov/">Alexander Masalov</a>,
              <a href="https://www.linkedin.com/in/jeffota/">Jeffrey Ota</a>,
              <a href="https://cs.stanford.edu/~chrismre/">Christopher R&eacute;</a>
              <br>
              <em>IEEE Intelligent Vehicles Symposium<a href="https://iv2019.org/"> (IEEE IV)</a></em>, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8814147&tag=1">Paper</a>
              <p></p>
              <p>We introduced weak supervision heuristics as a methodology to infer complex objects and situations by combining simpler outputs from current, state-of-the art object detectors.</p>
            </td>
          </tr>

      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Work Experience</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr onmouseout="box_stop()" onmouseover="box_start()">
    
        <td valign="center" width="75%">
          <p>
            Research Scientist Intern @ <a href="https://research.adobe.com/">Adobe Research</a>, San Jose, CA, Jun - Sept, 2023
            <br><br>

            Research Intern (Perception) @ <a href="https://waymo.com/?ncr">Waymo</a>, Mountain View, CA, Jun - Nov, 2022
            <br><br>

            Machine Learning Engineer @ <a href="https://www.vmware.com/">VMware</a>, Palo Alto, CA, Jun - Sept, 2019
            <br><br>
            
            Research Engineer  @ <a href="https://www.aqr.com/">AQR</a>, Greenwich, CT, 2016 - 2018
            <br><br>

            </a>
            <br>
          </p>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          Webpage template and source code from <a href="https://jonbarron.info">Jon Barron</a>. 
	    </font>
        </p>
        </td>
      </tr>
      </table>

    </td>
    </tr>
  </table>
  </body>
</html>
